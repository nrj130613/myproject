# -*- coding: utf-8 -*-
"""RNN contest2 รื้อ ด่วน.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1myDBtoWLQRO_kD0LDDnMc3Sm3NzVSiUe

# Recurrent Neural Network
"""

!pip install --upgrade --no-cache-dir gdown

!gdown 1RF697A7FCVvdbYtXGrmzupq04FluQh_f

!gdown 17upH5GLwuKJ3sZEzW0Zzdz63bkKw_V8Y

import pandas as pd
data_dev_set = pd.read_csv('dev_set.csv')

data_dev_set

import nltk
nltk.download('punkt')

with open('train.src.tok', 'r') as f:
    training_data = f.read().split('\n')

sentences = training_data[0:500000]

context_list = data_dev_set['context'][0:400].tolist()

len(context_list)

first_letter = data_dev_set['first letter'][0:400].tolist()

len(first_letter)

import numpy as np
from keras.models import Sequential
from keras.layers import Embedding, SimpleRNN, Dense, Input, TextVectorization, TimeDistributed
from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences

# Load a text corpus

split_point = len(sentences) * 4 // 5

train_set = sentences[:split_point]
dev_set = sentences[split_point:]

vocab_size = 68000
tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")
tokenizer.fit_on_texts(sentences)
train_set = tokenizer.texts_to_sequences(train_set)
dev_set = tokenizer.texts_to_sequences(dev_set)

len(tokenizer.word_index)

max_len = 35  # Sequence length to pad the outputs to.
train_set = pad_sequences(sequences=train_set, maxlen=max_len, padding='pre', truncating='pre')
dev_set = pad_sequences(sequences=dev_set, maxlen=max_len, padding='pre', truncating='pre')

dev_set

from keras.models import Sequential
from keras.layers import Embedding, GRU, Dense
def create_model(max_len, vocab_size, embedding_dim, hidden_dim):
  model = Sequential()
  # Create the layer.
  model.add(Input(shape=(max_len-1,), dtype='int32'))
  model.add(Embedding(vocab_size, embedding_dim, mask_zero=True))
  model.add(GRU(hidden_dim, return_sequences=True))
  model.add(TimeDistributed(Dense(vocab_size, activation="softmax")))
  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
  model.summary()
  return model

model = create_model(max_len, vocab_size, 200, 256)

history = model.fit(
    train_set[:,:-1],
    train_set[:,1:,None],
    batch_size=128,
    epochs=2,
    validation_data=(dev_set[:,:-1], dev_set[:,1:,None])
)

import matplotlib.pyplot as plt
def plot_history(history):
  plt.style.use('ggplot')
  plt.plot(history.history['loss'])
  plt.plot(history.history['val_loss'])
  plt.ylabel('loss')
  plt.xlabel('epoch')
  plt.legend(['train', 'dev'], loc='best')
  plt.show()
plot_history(history)

train_set[0]

prompt = "south korea and the united states on monday warned north korea to avoid provoking trouble as pyongyang ' s most senior defector spent his sixth"  # B B C trigram exist a lot. The only environment that C occurs
prompt_idx = tokenizer.texts_to_sequences([prompt])
prompt_idx = pad_sequences(sequences=prompt_idx, maxlen=max_len-1)
model.predict(prompt_idx)[0][-1]

prompt_idx

"""**ใช้ได้**"""

import random
from numpy.random import choice

def generate(row, tokenizer, predictor_model, end_token=None):
    prompt = row['context']
    first_letter = row['first letter']
    predicted_answer = []
    possible_words = []
    for i in range(max_len-1):
      prompt_idx = tokenizer.texts_to_sequences([prompt])
      prompt_idx = pad_sequences(sequences=prompt_idx, maxlen=max_len-1)
      probs = predictor_model.predict(prompt_idx)[0][-1]
      sorted_indices = [i for i, _ in sorted(enumerate(probs), key=lambda x: x[1], reverse=True)]
      for j in range(len(sorted_indices)):
        index = sorted_indices[j]
        if index in tokenizer.index_word:
            poss_word = tokenizer.index_word[index]
            possible_words.append(poss_word)

    for k in range(len(possible_words)):
      if possible_words[k][0] == first_letter:
        predicted_answer.append(possible_words[k])


        return possible_word

generate(context_list, tokenizer, model, first_letter)

generate("after agreeing to drastically cut its car import duties , taiwan on thursday won european union support for its bid to enter", tokenizer, model, 't')

data_test = data_dev_set[0:400]

def generate(row, end_token=None):
    prompt = row['context']
    first_letter = row['first letter']
    predicted_answer = []
    possible_words = []
    for i in range(max_len-1):
      prompt_idx = tokenizer.texts_to_sequences([prompt])
      prompt_idx = pad_sequences(sequences=prompt_idx, maxlen=max_len-1)
      probs = model.predict(prompt_idx)[0][-1]
      sorted_indices = [i for i, _ in sorted(enumerate(probs), key=lambda x: x[1], reverse=True)]
      for j in range(len(sorted_indices)):
        index = sorted_indices[j]
        if index in tokenizer.index_word:
            poss_word = tokenizer.index_word[index]
            possible_words.append(poss_word)

    for k in range(len(possible_words)):
      if possible_words[k][0] == first_letter:
        return possible_words[k]

data_test['predicted answer'] = data_test.apply(generate, axis=1)

data_test

answer_list = data_test['answer'].tolist()


predict_list = data_test['predicted answer'].tolist()

"""Compute acc"""

def accuracy(true_labels, predicted_labels):
    """
    Computes the accuracy of predicted labels given the true labels.

    Args:
        true_labels (list): A list of true labels.
        predicted_labels (list): A list of predicted labels.

    Returns:
        float: The accuracy of predicted labels, as a percentage.
    """
    correct_count = 0
    for i in range(len(true_labels)):
        if true_labels[i] == predicted_labels[i]:
            correct_count += 1
    acc = correct_count / len(true_labels) * 100
    return acc

acc = accuracy(answer_list, predict_list)
print(f"Accuracy: {acc:.2f}%")