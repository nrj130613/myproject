# -*- coding: utf-8 -*-
"""contest1 neural.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GwAIpcLdA_0oOURKRKmosTWum6b6AHNG

# Neural Network Basics

* Feedforward neural network
* Training neural networks effectively
"""

import numpy as np
import pandas as pd
from numpy.linalg import norm
from sklearn.metrics import classification_report, confusion_matrix
from IPython.display import Image, display_png
from gensim.models import word2vec, KeyedVectors
from keras.models import Sequential
from keras.layers import (
    Input, Embedding, Dense, Dropout, Flatten, GlobalAveragePooling1D, Conv1D, GlobalMaxPooling1D, BatchNormalization
)
from keras.utils import to_categorical, plot_model, pad_sequences
from keras.optimizers import RMSprop
import matplotlib.pyplot as plt
plt.style.use('ggplot')

from keras.models import Sequential
from keras.optimizers import Adam

#import for optimizer
from tensorflow.keras import layers
from tensorflow.keras import regularizers
from keras import optimizers

"""# Feedforward Neural Network

## 1. Load up data
"""

# load data
data = pd.read_csv('contest1_train.csv', encoding='utf-8')

# show data
data

import nltk
nltk.download('punkt')

data['tokenized'] = data['text'].apply(lambda x: '|'.join(nltk.word_tokenize(x)))
data['length'] = data['tokenized'].apply(lambda x: x.count('|'))

data

"""ส่วนใหญ่มีแค่ 100 คำ ไม่ต้องใช้ทั้งหมด"""

data.length.describe()

#dev.length.describe()

"""## 2. Load up the pre-trained word embeddings"""

!wget http://nlp.stanford.edu/data/glove.6B.zip

!unzip glove*.zip

!pip install --upgrade --no-cache-dir gdown

!pip install gdown
!gdown https://drive.google.com/uc?id=1k5zNhHJnXukYCtHwqNSXmwaziOX8JosX

from gensim.test.utils import get_tmpfile
from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec

# create temp file and save converted embedding into it
target_file = get_tmpfile('glove.6B.200d.txt')
glove2word2vec('glove.6B.200d.txt', target_file)

# load the converted embedding into memory
model = KeyedVectors.load_word2vec_format(target_file)

# save as binary data
model.save_word2vec_format('glove.6B.200d.bin', binary=True)

#load word embedding and data
w2v_model = KeyedVectors.load_word2vec_format('glove.6B.200d.bin',
                                              binary=True, unicode_errors='ignore')

vocab_size = len(w2v_model.vocab)
vector_dim = w2v_model.vector_size
# make weight matrix of word embedding, vocab size + 1 (for padding)
embedding_matrix = np.zeros((vocab_size+2, vector_dim), dtype="float32")
embedding_matrix[0] = np.zeros(vector_dim)

word_to_index = {word:i+1 for i, word in enumerate(w2v_model.vocab)}
# word to index dictionary, 0 for padding, UNKNOWN
word_to_index['PADDING'] = 0
word_to_index['<UNK>'] = len(word_to_index)

for i, word in enumerate(w2v_model.vocab):
    embedding_matrix[i+1] = w2v_model[word]

# vocabulary size of pre-trained model
vocab_size = len(w2v_model.vocab)
print('vocab size:', vocab_size)

# vector dimension
vector_dim = (len(w2v_model['the']))
print('vector dimension:', vector_dim)

# load data

from sklearn.model_selection import train_test_split
X = data.drop('aspectCategory', axis=1).values
y = data['aspectCategory'].values
# Assuming X contains the input features and y contains the corresponding labels
# Split the data into training and testing sets
train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)

# Split the training data into training and validation sets
train_x, dev_x, train_y, dev_y = train_test_split(train_x, train_y, test_size=0.2, random_state=42)

# Print the size of each subset
print(f"Training set size: {len(train_x)}")
print(f"Validation set size: {len(dev_x)}")
print(f"Test set size: {len(test_x)}")

train, dev_test = train_test_split(data, train_size=0.8, random_state=42)
dev, test = train_test_split(dev_test, test_size=0.2, random_state=42)

print(f"Training set size: {len(train)}")
print(f"Validation set size: {len(dev)}")
print(f"Test set size: {len(test)}")

test['aspectCategory'].value_counts()

test['polarity'].value_counts()



test

"""## 3. Convert words into indices and pad + truncate sequences"""

def convert_words(df, word_to_index, max_length):
  tokens = df['tokenized'].apply(lambda x: x.split('|'))
  list_of_list_of_indices = list(tokens.map(lambda x: [word_to_index.get(word, word_to_index['<UNK>']) for word in x]))
  return pad_sequences(list_of_list_of_indices, max_length, padding='post', value=word_to_index['PADDING'], truncating='post')

# max length (กำหนดเอง)
max_len = 100
train_x = convert_words(train, word_to_index, max_len)
dev_x = convert_words(dev, word_to_index, max_len)
num_classes = 5

"""## 4. Mapping labels"""

def get_label(df):
  star_to_label = {'ambience':0, 'anecdotes/miscellaneous':1, 'food':2, 'price':3, 'service':4}
  # apply functions & convert to np.array
  label = np.array(df['aspectCategory'].replace(star_to_label).tolist())
  df['label'] = label
  return to_categorical(label, num_classes=5)

# label : one-hot vector
train_y = get_label(train)
dev_y = get_label(dev)

# check the shape
print('input train:', train_x.shape)
print('input dev:', dev_x.shape)
print('label train:', train_y.shape)
print('label dev:',dev_y.shape)

"""## 5. Train the model

## Deep Average Net
"""

def make_deep_average_net(print_model=True):
    # instantiation
    model = Sequential()

    # add embedding layer
    model.add(Embedding(input_dim=embedding_matrix.shape[0],
                        input_length=max_len,
                        output_dim=embedding_matrix.shape[1],
                        weights=[embedding_matrix],
                        mask_zero=True,
                        trainable=False))

    model.add(BatchNormalization())

    # average
    model.add(GlobalAveragePooling1D())

    # add hidden layer
    model.add(Dense(100, activation='relu'))

    model.add(Dropout(0.3))

    model.add(Dense(100, activation='relu'))

    model.add(Dropout(0.5))

    # add output layer
    model.add(Dense(num_classes, activation='softmax'))

    lr = 0.001
    optimizer = Adam(lr=lr)

    # compile model
    model.compile(optimizer="Adam", loss="categorical_crossentropy", metrics=["accuracy"])
    if print_model:
        model.summary()
        plot_model(model, show_shapes=True,to_file='model.png')
        display_png(Image('model.png'))
    return model

model = make_deep_average_net()

# train
history = model.fit(train_x, train_y, batch_size=64, epochs=8, validation_data=(dev_x, dev_y))

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'dev'], loc='best')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'dev'], loc='best')
plt.show()

"""## 6. Evaluate the model"""

prediction = [np.argmax(x) for x in model.predict(dev_x)]
print(classification_report(dev['label'], prediction))

"""## CNN"""

def make_cnn_model(print_model=True):
    filters = 300 #number of filters in your Convnet
    kernel_size = 3 # a window size of 3 tokens
    hidden_dims = 100 #number of neurons at the normal feedforward NN

    cnn_model = Sequential()
    cnn_model.add(Embedding(input_dim=embedding_matrix.shape[0],
                        input_length=max_len,
                        output_dim=embedding_matrix.shape[1],
                        weights=[embedding_matrix],
                        mask_zero=True,
                        trainable=False))

    cnn_model.add(Conv1D(filters,
                        kernel_size,
                        input_shape=(max_len, embedding_matrix.shape[1]),
                        activation='relu',))

    cnn_model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01), bias_regularizer=regularizers.l2(0.01)))

    cnn_model.add(GlobalMaxPooling1D())
    #GlobalMaxPooling1D(n) default = 2.
    cnn_model.add(Dense(hidden_dims, activation='relu'))
    cnn_model.add(Dropout(0.4))
    cnn_model.add(Dense(5, activation='softmax'))

    # compile model
    cnn_model.compile(optimizer=optimizers.RMSprop(learning_rate=0.001),
              loss="categorical_crossentropy", metrics=["accuracy"])
    if print_model:
        cnn_model.summary()
        plot_model(cnn_model, show_shapes=True,to_file='model.png')
        display_png(Image('model.png'))
    return cnn_model

cnn_model = make_cnn_model()

# train
cnn_history = cnn_model.fit(train_x, train_y, batch_size=128, epochs=7, validation_data=(dev_x, dev_y))

plt.plot(cnn_history.history['accuracy'])
plt.plot(cnn_history.history['val_accuracy'])
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'dev'], loc='best')
plt.show()

plt.plot(cnn_history.history['loss'])
plt.plot(cnn_history.history['val_loss'])
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'dev'], loc='best')
plt.show()

prediction = [np.argmax(x) for x in cnn_model.predict(dev_x)]
print(classification_report(dev['label'], prediction))

cnn_model.evaluate(dev_x, dev_y)[1]

"""## Hyperparameter"""

from keras import optimizers, regularizers
model.compile(optimizer=optimizers.Adagrad(learning_rate=0.001),
              loss="categorical_crossentropy", metrics=["accuracy"])

Dense(250, activation='relu',
      kernel_regularizer=regularizers.l2(l2=1e-5),
      bias_regularizer=regularizers.l2(1e-5))

model.add(Dropout(0.4))