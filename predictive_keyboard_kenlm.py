# -*- coding: utf-8 -*-
"""test ken.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/177yANp0XrJ8o_d1P0CcLbsPZYAOB40DG
"""

!pip install --upgrade --no-cache-dir gdown

"""Full training set"""

!gdown 1RF697A7FCVvdbYtXGrmzupq04FluQh_f

"""dev set"""

!gdown 17upH5GLwuKJ3sZEzW0Zzdz63bkKw_V8Y

"""5000 sentences"""

!gdown 1JMpz4sd5wwyFMKsaMwBZyDilBRo-xyHM

"""6000 sentences"""

!gdown 1GtEXbk3Rm3DxvkRJM51ztoqyLy9NdTjg

"""7000 sentences"""

!gdown 156L5S2Bq2O3MksRxRHh2oBSu6EDPpjl6

"""8000 sentences"""

!gdown 1hVixY6p-t6jWJiWBHgJJXGarmXXwUbTS

"""100000 sentences"""

!gdown 1MEJnq3Hr8fwi9L2gnnWrTwfHAcpoIIuv

"""10000 sentences"""

!gdown 1o34OrJMSydjF63qnIJzrXIwZv7Nma7ms

"""12000 sentences"""

!gdown 1ohSuYm2ApzIPUUiHDG3orM6xpqaWqw2N

"""50000 sentences"""

!gdown 1q9Y_ZKk11EvPODwsmu0z-foRetL6fIda

"""100000 sentences"""

!gdown 1la5gYyESKrNLcEJlp_hb5wTBq3UPlkFI

"""200000 sentences"""

!gdown 1nUjXBGfErMLTduyGUy2eijeOpdB2K9KH

"""500000 sentences"""

!gdown 1T8t_UBWyez2YV7OGKoXN7aM5aGAmVP7C

!gdown 1D7Smf6dMAt7XwfRTxdYWeHkY_Slo9Fs4

"""2M sentences

"""

!gdown 1Hv9WC2SstPa0F-HbXHw4jAKSRaBgKG0r

import pandas as pd
dev_set = pd.read_csv('dev_set.csv')

dev_set

"""KEN LM"""

!wget -O - https://kheafield.com/code/kenlm.tar.gz |tar xz
!mv kenlm kenlm_tool
!mkdir kenlm_tool/build

# Commented out IPython magic to ensure Python compatibility.
# %cd kenlm_tool/build
!cmake ..
!make -j2

!pip install https://github.com/kpu/kenlm/archive/master.zip

"""train kenlm"""

!bin/lmplz -o 5 --text 2M.src.tok --arpa fivegram.arpa

ls

import kenlm
model = kenlm.Model('fivegram.arpa')

import math
import pandas as pd
def print_score(model, s):
  tokens = s.split(' ')
  log_score = 0.0
  rows = []
  for i, (logprob, length, oov) in enumerate(model.full_scores(s)):
    if i < len(tokens):
      row = {'token': tokens[i], 'probability': math.exp(logprob), 'Is OOV?': oov}
    else:
      row = {'token': 'END', 'probability': math.exp(logprob), 'Is OOV?': oov}
    rows.append(row)
    log_score += logprob
  print ('Log probability = ', log_score)
  return pd.DataFrame(rows)

with open('2M.src.tok', 'r') as f:
    vocab = f.read().split()
len(vocab)

with open('train.src.tok', 'r') as f:
    training_data = f.read().split('\n')

sentences = [s.split(' ') for s in training_data[0:100000]]

sentences

first_letter_list = dev_set['first letter'][0:500].tolist()

len(first_letter_list)

context_list = dev_set['context'][0:500].tolist()

len(context_list)

best_words = []
max_prob_so_far = -float('inf')
best_word = None
for context in context_list:
    for first_letter in first_letter_list:
        for word in vocab:
            if word[0] == first_letter:
                sentence = context + ' ' + word
                prob = model.score(sentence)
                if prob > max_prob_so_far:
                    best_word = word
                    max_prob_so_far = prob
    best_words.append(best_word)

best_word

def predict_word(context_list, first_letter_list):
  best_words = []
  max_prob_so_far = -float('inf')
  best_word = None
  for context in context_list:
      for first_letter in first_letter_list:
          for word in vocab:
              if word[0] == first_letter:
                  sentence = context + ' ' + word
                  prob = model.score(sentence)
                  if prob > max_prob_so_far:
                      best_word = word
                      max_prob_so_far = prob
      best_words.append(best_word)
  return best_word

predict_word(context_list, first_letter_list)

!gdown 17rntxqy9vPR-Z3kPEC64j6AtwTXKCazI

import pandas as pd
test_set = pd.read_csv('test_set_no_answer.csv')

test_set

data_test = dev_set[0:500]

# define a function to apply to each row of the DataFrame
def get_best_word(row):
    context = row['context']
    first_letter = row['first letter']
    max_prob_so_far = -float('inf')
    best_word = None
    for word in vocab:
        if word[0] == first_letter:
            sentence = context + ' ' + word
            prob = model.score(sentence)
            if prob > max_prob_so_far:
                best_word = word
                max_prob_so_far = prob
    return best_word

# apply the function to each row of the DataFrame
test_set['predicted answer'] = test_set.apply(get_best_word, axis=1)

# view the resulting DataFrame

data_test

from google.colab import files
dev_set.to_csv('1000000s_predict.csv', encoding = 'utf-8-sig')
files.download('1000000s_predict.csv')

from google.colab import drive
drive.mount('/content/drive')

best_words = data_test['best_word'].tolist()

answers = data_test['answer'].tolist()

def accuracy(true_labels, predicted_labels):
    """
    Computes the accuracy of predicted labels given the true labels.

    Args:
        true_labels (list): A list of true labels.
        predicted_labels (list): A list of predicted labels.

    Returns:
        float: The accuracy of predicted labels, as a percentage.
    """
    correct_count = 0
    for i in range(len(true_labels)):
        if true_labels[i] == predicted_labels[i]:
            correct_count += 1
    acc = correct_count / len(true_labels) * 100
    return acc

acc = accuracy(answers, best_words)
print(f"Accuracy: {acc:.2f}%")

# create a dictionary where the keys are the first letters of the words
vocab_dict = {}
for word in vocab:
    first_letter = word[0]
    if first_letter not in vocab_dict:
        vocab_dict[first_letter] = []
    vocab_dict[first_letter].append(word)

# define a function to apply to each row of the DataFrame
def select_best_word(row):
    context = row['context']
    first_letter = row['first letter']
    max_prob_so_far = -float('inf')
    best_word = None
    if first_letter in vocab_dict:
        for word in vocab_dict[first_letter]:
            sentence = context + ' ' + word
            prob = model.score(sentence)
            if prob > max_prob_so_far:
                best_word = word
                max_prob_so_far = prob
    return best_word

# apply the function to each row of the DataFrame
dev_set['best_word'] = dev_set.apply(select_best_word, axis=1)

# Get the log probability of all possible next words given the context
next_word_probs = model.full_scores(context)

# Compute the probabilities of the candidate next words
for context in context_list:
  for first_letter in first_letter_list:
    best_candidate_list = []
    for candidate in vocab:
        # Concatenate the candidate word to the context
      if candidate[0] == first_letter:
        candidate_context = f"{context} {candidate}"
        candidate_log_prob = model.score(candidate_context)
        candidate_prob = math.exp(candidate_log_prob)
        if candidate_prob > best_candidate_prob:
          best_candidate = candidate
          best_candidate_prob = candidate_prob

      best_candidate_list.append(best_candidate)
print(best_candidate_list)

# Define the context
context = context_list
first_letter = first_letter_list
best_candidate = ""
best_candidate_prob = 0

# Get the log probability of the context
for context in context_list:
  context_log_prob = model.score(context)

# Compute the probability of each candidate next word using the backoff property of the language model

  for first_letter in first_letter_list:
    best_candidate_list = []
    for candidate in vocab:
        # Concatenate the candidate word to the context
        candidate_context = f"{context} {candidate}"

        # Get the log probability of the candidate word given the candidate context using the backoff property
        candidate_log_prob = model.full_scores(candidate, context_log_prob=context_log_prob)

        # Compute the probability of the candidate word by exponentiating the log probability
        candidate_prob = math.exp(candidate_log_prob)

        if candidate_prob > best_candidate_prob:
            best_candidate = candidate
            best_candidate_prob = candidate_prob

            best_candidate_list.append(best_candidate)

print(best_candidate_list)

# Define the context
context = "after agreeing to drastically cut its car import duties , taiwan on thursday won european union support for its bid to enter"
first_letter = 't'

# Get the log probability of all possible next words given the context
next_word_probs = model.full_scores(context)

# Compute the probabilities of the candidate next words
best_candidate_list = []
best_candidate = ""
best_candidate_prob = 0
for candidate in vocab:
        # Concatenate the candidate word to the context
  if candidate[0] == first_letter:
    candidate_context = f"{context} {candidate}"
    candidate_log_prob = model.score(candidate_context)
    candidate_prob = math.exp(candidate_log_prob)
    if candidate_prob > best_candidate_prob:
      best_candidate = candidate
      best_candidate_prob = candidate_prob

      best_candidate_list.append(best_candidate)
print(best_candidate_list)

print_score(model, 'I look forward to meeting you')

print_score(model, 'I look forward to meet you')

print_score(model, 'The president ceases trading to China yesterday')

print_score(model, 'The president ceased trading with China yesterday')

with open('train.src.tok', 'r') as f:
    training_data = f.read().split('\n')


tokens = []
for s in training_data[0:3]:
  sublist = s.split(' ')
  for tok in sublist:
    tokens.append(tok)

from collections import Counter

s = 'south korea and the united states on monday was'
tokens = s.split(' ')
counter = Counter(tokens)

for i in range(len(tokens)- 3 - 1):
       context = tuple(tokens[i:(i+3-1)])
       word = tokens[(i+3-1)]
       if context not in counter:
         counter[context] = Counter()
       counter[context][word] += 1

counter[context].most_common()

import kenlm
from collections import Counter

# Load the training corpus
with open('train.src.tok', 'r') as f:
    training_data = f.read().split()

# Function to generate a list of candidate words for a given input prefix
def get_candidate_words(prefix, model):
    candidates = []
    prefix_score = model.score(prefix, bos=True, eos=False)
    for score_tuple in model.full_scores(prefix):
      print(score_tuple)
      score, word = score_tuple[:2]
      candidates.append(word)
    return candidates

# Function to rank candidate words by frequency in the training data
def rank_candidates(candidates, training_data):
    freq = Counter(training_data)
    ranked_candidates = sorted(candidates, key=lambda w: freq[w], reverse=True)
    return ranked_candidates

# Example usage
prefix = 'a national'
candidates = get_candidate_words(prefix, model)
ranked_candidates = rank_candidates(candidates, training_data)

# Print the top 3 candidate words
print(ranked_candidates[:3])

ranked_candidates

for x in ranked_candidates:
  print(training_data[x])